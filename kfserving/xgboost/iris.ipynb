{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Install necessary packages\n",
    "\n",
    "We can install the necessary packages by either running `pip install --user <package_name>` or include everything in a `requirements.txt` file and run `pip install --user -r requirements.txt`.\n",
    "\n",
    "> NOTE: Do not forget to use the `--user` argument. It is necessary if you want to use Kale to transform this notebook into a Kubeflow pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (0.23.2)\n",
      "Requirement already satisfied: xgboost==0.90 in /home/jovyan/.local/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (0.90)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->-r requirements.txt (line 1)) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->-r requirements.txt (line 1)) (0.17.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->-r requirements.txt (line 1)) (1.5.3)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->-r requirements.txt (line 1)) (1.18.5)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install --user -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Imports\n",
    "\n",
    "In this section we import the packages we need for this example. Make it a habbit to gather your imports in a single place. It will make your life easier if you are going to transform this notebook into a Kubeflow pipeline using Kale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "imports"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert float(xgb.__version__) < 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Project hyper-parameters\n",
    "\n",
    "In this cell, we define the different hyper-parameters variables. Defining them in one place makes it easier to experiment with their values and also facilitates the execution of HP Tuning experiments using Kale and Katib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "pipeline-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "ETA = .3\n",
    "MAX_DEPTH = 3\n",
    "OBJECTIVE = \"multi:softprob\"\n",
    "STEPS = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load and preprocess data\n",
    "\n",
    "In this section, we load and process the dataset to get it in a ready-to-use form by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "block:load_transform_data"
    ]
   },
   "outputs": [],
   "source": [
    "x, y = datasets.load_iris(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_trn, x_tst, y_trn, y_tst = train_test_split(x, y, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "D_trn = xgb.DMatrix(x_trn, label=y_trn)\n",
    "D_tst = xgb.DMatrix(x_tst, label=y_tst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Define and train the model\n",
    "\n",
    "We are now ready to define our model. In this example, we use the Extreme Gradient Boosting algorithm inmplemented by [XGBoost](https://xgboost.ai/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "block:train_model",
     "prev:load_transform_data"
    ]
   },
   "outputs": [],
   "source": [
    "param = {\"eta\": ETA, \n",
    "         \"max_depth\": MAX_DEPTH,  \n",
    "         \"objective\": OBJECTIVE,  \n",
    "         \"num_class\": 3} \n",
    "\n",
    "steps = STEPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = xgb.train(param, D_trn, steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save_model(\"model.bst\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Evaluate the model\n",
    "\n",
    "Finally, we are ready to evaluate the model using the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "block:evaluate_model",
     "prev:load_transform_data",
     "prev:train_model"
    ]
   },
   "outputs": [],
   "source": [
    "preds = model.predict(D_tst)\n",
    "max_preds = np.asarray([np.argmax(line) for line in preds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "precision = precision_score(y_tst, max_preds, average='macro')\n",
    "recall = recall_score(y_tst, max_preds, average='macro')\n",
    "f1 = f1_score(y_tst, max_preds, average='macro')\n",
    "accuracy = accuracy_score(y_tst, max_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Serving\n",
    "\n",
    "We can deploy the model and use it just like any other web service using KFServing. All we need is to create the necessary configuration file. Take a look in the `xgboost.yaml` file. We need to specify an `xgboost` predictor and make the `storageUri` parameter point to the folder when the `.bst` model file is stored.\n",
    "\n",
    "> Note: The name of the file should be `model.bst`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferenceservice.serving.kubeflow.org/xgboost-example unchanged\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f xgboost.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data = {\n",
    "    \"instances\": [\n",
    "        [6.8, 2.8, 4.8, 1.4],\n",
    "        [5.1, 3.5, 1.4, 0.2]\n",
    "    ]\n",
    "}\n",
    "\n",
    "headers = {\"content-type\": \"application/json\", \"Host\": \"xgboost-example.kubeflow-user.svc.cluster.local\"}\n",
    "response = requests.post(\"http://cluster-local-gateway.istio-system/v1/models/xgboost-example:predict\", json=data, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"predictions\": [[0.007211383432149887, 0.9620232582092285, 0.030765343457460403], [0.9892104864120483, 0.006796461995691061, 0.003993045538663864]]}'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Pipeline metrics\n",
    "\n",
    "In the last cell of the Notebook, we print the pipeline metrics. These will be picked up by Kubeflow Pipelines, which will make them available through its UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": [
     "pipeline-metrics"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8925925925925925\n",
      "0.8977272727272728\n",
      "0.8935574229691877\n",
      "0.9\n"
     ]
    }
   ],
   "source": [
    "print(precision)\n",
    "print(recall)\n",
    "print(f1)\n",
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kubeflow_notebook": {
   "autosnapshot": true,
   "docker_image": "dimpo/jupyter-kale-cpu:v0.5.1-xgboost-v0.0.1",
   "experiment": {
    "id": "new",
    "name": "iris-experiment"
   },
   "experiment_name": "iris-experiment",
   "katib_metadata": {
    "algorithm": {
     "algorithmName": "grid",
     "algorithmSettings": [
      {
       "name": "random_state",
       "value": "10"
      },
      {
       "name": "acq_optimizer",
       "value": "auto"
      },
      {
       "name": "acq_func",
       "value": "gp_hedge"
      },
      {
       "name": "base_estimator",
       "value": "GP"
      }
     ]
    },
    "maxFailedTrialCount": 3,
    "maxTrialCount": 15,
    "objective": {
     "additionalMetricNames": [
      "precision",
      "recall",
      "f1"
     ],
     "goal": 1,
     "objectiveMetricName": "accuracy",
     "type": "maximize"
    },
    "parallelTrialCount": 3,
    "parameters": [
     {
      "feasibleSpace": {
       "max": "0.3",
       "min": "0.1",
       "step": "0.1"
      },
      "name": "ETA",
      "parameterType": "double"
     },
     {
      "feasibleSpace": {
       "max": "6",
       "min": "2",
       "step": "1"
      },
      "name": "MAX_DEPTH",
      "parameterType": "int"
     },
     {
      "feasibleSpace": {
       "list": [
        "multi:softprob",
        "multi:softmax"
       ]
      },
      "name": "OBJECTIVE",
      "parameterType": "categorical"
     },
     {
      "feasibleSpace": {
       "max": "100",
       "min": "20",
       "step": "10"
      },
      "name": "STEPS",
      "parameterType": "int"
     }
    ]
   },
   "katib_run": true,
   "pipeline_description": "Train an XGBoost model on the Iris dataset",
   "pipeline_name": "iris-pipeline",
   "snapshot_volumes": true,
   "steps_defaults": [
    "label:access-ml-pipeline:true",
    "label:access-rok:true"
   ],
   "volumes": [
    {
     "annotations": [],
     "mount_point": "/home/jovyan",
     "name": "workspace-examples-serving-bgsd4h5em",
     "size": 5,
     "size_type": "Gi",
     "snapshot": false,
     "type": "clone"
    },
    {
     "annotations": [],
     "mount_point": "/home/jovyan/data",
     "name": "data-8lbdjmzdh",
     "size": 10,
     "size_type": "Gi",
     "snapshot": false,
     "type": "clone"
    }
   ]
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
