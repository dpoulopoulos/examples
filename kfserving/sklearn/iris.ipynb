{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Install necessary packages\n",
    "\n",
    "We can install the necessary packages by either running `pip install --user <package_name>` or include everything in a `requirements.txt` file and run `pip install --user -r requirements.txt`.\n",
    "\n",
    "> NOTE: Do not forget to use the `--user` argument. It is necessary if you want to use Kale to transform this notebook into a Kubeflow pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn==0.20.3\n",
      "  Downloading scikit_learn-0.20.3-cp36-cp36m-manylinux1_x86_64.whl (5.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.4 MB 2.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.20.3->-r requirements.txt (line 1)) (1.18.5)\n",
      "Requirement already satisfied: scipy>=0.13.3 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.20.3->-r requirements.txt (line 1)) (1.5.3)\n",
      "Installing collected packages: scikit-learn\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.22.2\n",
      "    Uninstalling scikit-learn-0.22.2:\n",
      "      Successfully uninstalled scikit-learn-0.22.2\n",
      "Successfully installed scikit-learn-0.20.3\n"
     ]
    }
   ],
   "source": [
    "!pip3 install --user -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Imports\n",
    "\n",
    "In this section we import the packages we need for this example. Make it a habbit to gather your imports in a single place. It will make your life easier if you are going to transform this notebook into a Kubeflow pipeline using Kale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "imports"
    ]
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "import sklearn\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert sklearn.__version__ == \"0.20.3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Project hyper-parameters\n",
    "\n",
    "In this cell, we define the different hyper-parameters variables. Defining them in one place makes it easier to experiment with their values and also facilitates the execution of HP Tuning experiments using Kale and Katib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "pipeline-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "N_ESTIMATORS = 500\n",
    "MAX_DEPTH = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load and preprocess data\n",
    "\n",
    "In this section, we load and process the dataset to get it in a ready-to-use form by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "block:load_transform_data"
    ]
   },
   "outputs": [],
   "source": [
    "x, y = datasets.load_iris(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_trn, x_tst, y_trn, y_tst = train_test_split(x, y, test_size=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Define and train the model\n",
    "\n",
    "We are now ready to define our model. In this example, we use the scikit-learn implementation of Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "block:train_model",
     "prev:load_transform_data"
    ]
   },
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=N_ESTIMATORS,\n",
    "                               max_depth=MAX_DEPTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=2, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_trn, y_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "with open(\"model.joblib\", \"wb\") as f:\n",
    "    joblib.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Evaluate the model\n",
    "\n",
    "Finally, we are ready to evaluate the model using the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [
     "block:evaluate_model",
     "prev:load_transform_data",
     "prev:train_model"
    ]
   },
   "outputs": [],
   "source": [
    "preds = model.predict(x_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "precision = precision_score(y_tst, preds, average='macro')\n",
    "recall = recall_score(y_tst, preds, average='macro')\n",
    "f1 = f1_score(y_tst, preds, average='macro')\n",
    "accuracy = accuracy_score(y_tst, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Serving\n",
    "\n",
    "We can deploy the model and use it just like any other web service using KFServing. All we need is to create the necessary configuration file. Take a look in the `sklearn.yaml` file. We need to specify an `sklearn` predictor and make the `storageUri` parameter point to the location where the `.joblib` model file is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferenceservice.serving.kubeflow.org/sklearn-iris created\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f sklearn.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "data = {\n",
    "    \"instances\": [\n",
    "        [6.8, 2.8, 4.8, 1.4],\n",
    "        [5.1, 3.5, 1.4, 0.2]\n",
    "    ]\n",
    "}\n",
    "\n",
    "headers = {\"content-type\": \"application/json\", \"Host\": \"sklearn-iris.kubeflow-user.svc.cluster.local\"}\n",
    "response = requests.post(\"http://cluster-local-gateway.istio-system/v1/models/sklearn-iris:predict\", json=data, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"predictions\": [1, 0]}'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Pipeline metrics\n",
    "\n",
    "In the last cell of the Notebook, we print the pipeline metrics. These will be picked up by Kubeflow Pipelines, which will make them available through its UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": [
     "pipeline-metrics"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.923076923076923\n",
      "0.9285714285714285\n",
      "0.9165217391304349\n",
      "0.9\n"
     ]
    }
   ],
   "source": [
    "print(precision)\n",
    "print(recall)\n",
    "print(f1)\n",
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kubeflow_notebook": {
   "autosnapshot": true,
   "docker_image": "",
   "experiment": {
    "id": "new",
    "name": "iris-experiment"
   },
   "experiment_name": "iris-experiment",
   "katib_metadata": {
    "algorithm": {
     "algorithmName": "grid",
     "algorithmSettings": [
      {
       "name": "random_state",
       "value": "10"
      },
      {
       "name": "acq_optimizer",
       "value": "auto"
      },
      {
       "name": "acq_func",
       "value": "gp_hedge"
      },
      {
       "name": "base_estimator",
       "value": "GP"
      }
     ]
    },
    "maxFailedTrialCount": 3,
    "maxTrialCount": 15,
    "objective": {
     "additionalMetricNames": [
      "precision",
      "recall",
      "f1"
     ],
     "goal": 1,
     "objectiveMetricName": "accuracy",
     "type": "maximize"
    },
    "parallelTrialCount": 3,
    "parameters": [
     {
      "feasibleSpace": {
       "max": "0.3",
       "min": "0.1",
       "step": "0.1"
      },
      "name": "ETA",
      "parameterType": "double"
     },
     {
      "feasibleSpace": {
       "max": "6",
       "min": "2",
       "step": "1"
      },
      "name": "MAX_DEPTH",
      "parameterType": "int"
     },
     {
      "feasibleSpace": {
       "list": [
        "multi:softprob",
        "multi:softmax"
       ]
      },
      "name": "OBJECTIVE",
      "parameterType": "categorical"
     },
     {
      "feasibleSpace": {
       "max": "100",
       "min": "20",
       "step": "10"
      },
      "name": "STEPS",
      "parameterType": "int"
     }
    ]
   },
   "katib_run": false,
   "pipeline_description": "Train a Random Forest classifier on the Iris dataset",
   "pipeline_name": "iris-pipeline",
   "snapshot_volumes": true,
   "steps_defaults": [
    "label:access-ml-pipeline:true",
    "label:access-rok:true"
   ],
   "volumes": [
    {
     "annotations": [],
     "mount_point": "/home/jovyan",
     "name": "workspace-examples-serving-bgsd4h5em",
     "size": 5,
     "size_type": "Gi",
     "snapshot": false,
     "type": "clone"
    },
    {
     "annotations": [],
     "mount_point": "/home/jovyan/data",
     "name": "data-8lbdjmzdh",
     "size": 10,
     "size_type": "Gi",
     "snapshot": false,
     "type": "clone"
    }
   ]
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
